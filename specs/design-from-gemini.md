# **Architectural Blueprint for a Zero-Compaction Recursive Language Model Extension in Pi Using TypeScript**

## **1\. Introduction: The Paradigm Shift in Context Management and Autoregressive Scaling**

Despite the rapid and continuous advancements in the reasoning capabilities, instruction-following precision, and tool-use proficiency of modern Large Language Models (LLMs), processing massive inputs remains a fundamental and pervasive computational bottleneck. Frontier models, such as GPT-4o and Claude 3.5 Sonnet, continuously expand their advertised context windows, frequently boasting capacities of 128,000 to 200,000 tokens.1 However, empirical evidence systematically demonstrates that these models inevitably suffer from a phenomenon widely identified as "context rot".3 As the length of the prompt grows into the tens or hundreds of thousands of tokens, the model's ability to reliably recall specific data points, cross-reference distant conceptual nodes, and maintain coherent reasoning trajectories over that information degrades significantly.5

Traditional scaffolding strategies and engineering architectures have historically attempted to mitigate this structural limitation through external data manipulation prior to model ingestion. Retrieval-augmented generation (RAG), vector-based semantic indexing, and active context compaction algorithms serve as the industry standards.1 These methods operate by summarizing older conversational turns, filtering data through BM25 or cosine similarity searches, and aggressively discarding text to make room for new content.3 However, these methods result in irreversible information loss. They systematically prove inadequate for heavy, complex reasoning tasks where granular details distributed across a massive document, a sprawling multi-file codebase, or an extensive log repository are strictly necessary for accurate task completion.3

To resolve this persistent architectural flaw, researchers from MIT CSAIL introduced Recursive Language Models (RLMs) in late 2025\.7 Rather than relying on linear context scaling and hardware-intensive attention mechanisms, the RLM architecture introduces a profound conceptual pivot: it treats massive prompts not as static input tokens to be consumed whole by the LLM, but as a dynamic, external, and searchable environment.7 The primary language model is wrapped in a persistent execution environment—typically a Read-Eval-Print Loop (REPL)—where the vast context is stored purely as an accessible programmatic variable.5 Instead of reading the entire context directly into its attention mechanism, the model generates executable code to programmatically inspect, slice, transform, and recursively process the data.5

This comprehensive technical report presents an exhaustive architectural blueprint for implementing a native, TypeScript-based Recursive Language Model extension for the pi coding agent framework. By utilizing the native @mariozechner/pi-coding-agent extension API, the event hooks system, and the terminal user interface (TUI) widget API, this design enables the agent to recursively process codebases and documents of theoretically unbounded size. The central, unyielding objective of this architecture is to ensure that the primary pi agent never exceeds its optimal context threshold, entirely eliminating the need for the system's native context compaction algorithms to trigger. To maximize operational efficiency and maintain native memory pointers, the entire architecture utilizes TypeScript, the implementation language of the host pi environment, eradicating the computational overhead traditionally associated with switching between disparate programming languages and serializing data across process boundaries. Furthermore, in accordance with the project constraints, the stringent security and OS-level guardrail requirements typically necessary for arbitrary code execution environments are deliberately omitted, assuming that the host container or external infrastructure will independently manage computational safety.

### **1.1 Mathematical Formalisms of Recursive Autoregressive Models**

To engineer a robust software architecture capable of zero-compaction reasoning, the theoretical and mathematical foundations of the Recursive Language Model must first be meticulously formalized. In a standard autoregressive language model family ![][image1], the system maps a sequence of embedded tokens to the embedding of the next predicted token. The system receives a user query ![][image2] and an associated context sequence ![][image3], effectively executing the monolithic black-box call ![][image4] to generate an output distribution.4 The recursive nature of standard transformer models is typically captured by defining the function as reusing shared transformer architecture across time steps via weight sharing, but fundamentally binding all historical data within the sequence length ![][image5].11

The Recursive Language Model framework systematically replaces this direct, linear token mapping with a vastly more expressive function ![][image6] defined over an external, stateful environment ![][image7].4 The environment ![][image7] holds the complete context ![][image8] in its isolated memory, completely shielded from the root model's direct token context window. Rather than outputting the final textual response immediately, the model generates a programmatic sequence or program ![][image9] that interacts with the variables held within ![][image7].12

Crucially, the environment provides a symbolic recursive tool call. When the model determines that a specific partition of the data requires deeper reasoning, it invokes this tool, which spawns an entirely isolated sub-RLM instance. This child instance is characterized by three distinct properties: a newly derived sub-query ![][image10] generated by the root model's programmatic logic; a transformed, highly specific slice of the context ![][image11] extracted from the broader data; and its own isolated execution sub-environment ![][image12].4

This recursive depth can theoretically scale to arbitrary levels. The final output from the recursive callee is ultimately returned as a symbolic variable back into the environment ![][image7] of the original caller.4 This mechanism ensures that the parent model's working memory remains completely unpolluted by the raw processing data, observing only the high-level semantic synthesis. This operational methodology relies strictly on symbolic recursion through executable code rather than standard tool-call verbalization, resolving the profound limitation where an LLM cannot explicitly verbalize ![][image13] discrete tool calls to process tens of thousands of individual data chunks.13

### **1.2 Inference-Time Scaling Laws and Economic Trade-offs**

The implementation of RLMs validates a fundamental shift in the economic theory of artificial intelligence: Inference-Time Scaling Laws. Prior to these developments, the prevailing dogma centered almost entirely on training scaling, operating under the assumption that greater data volumes and compute clusters during the training phase inherently yielded a smarter model.14 The RLM architecture proves that intelligence can be dynamically scaled on the fly by deliberately increasing compute time during the generation phase.

By orchestrating recursive sub-calls, an RLM trades inference latency for profound analytical depth.14 It operates as a Pareto-efficient system; rather than utilizing a massively expensive "genius" frontier model (which incurs prohibitive costs when processing millions of tokens in a single prompt and often suffers from context rot regardless), the RLM leverages a methodical "medium" model to iteratively grep, loop, and digest the data.14 The recursive calls can be efficiently parallelized—executing simultaneous analyses across disparate data chunks—and routed to significantly cheaper, lower-parameter models (such as Claude 3.5 Haiku or Gemini Flash), radically altering the cost profile of complex reasoning tasks.1 The TS-RLM extension proposed herein capitalizes on this economic reality by integrating model-routing parameters directly into the recursive execution loop.

## **2\. Comparative Analysis of Existing Long-Context Ecosystems**

To design an optimal, native TypeScript extension for the pi coding agent, it is necessary to rigorously evaluate existing implementations across different software ecosystems. The core concept of context folding and recursive sub-querying has been implemented through various scaffolding techniques over recent iterations, each presenting distinct architectural advantages and fatal bottlenecks.

### **2.1 The Claude Code Baseline and Agent Teams**

Anthropic's official Claude Code CLI represents a highly sophisticated baseline for terminal-native agentic development.16 It possesses deep integrations with local file systems, robust model context protocols, and specialized workflow capabilities. To combat the degradation of context over long sessions, Claude Code utilizes a combination of strategies, prominently featuring "Agent Teams" and sub-agents.17

In the Claude ecosystem, sub-agents are defined via Markdown files with YAML frontmatter dictating specific tools and instructions.18 The root agent orchestrates tasks by explicitly invoking these sub-agents via the "Task" tool.19 While this successfully isolates the context window—allowing a child agent to explore a codebase and return only a concise summary to the parent 20—it fundamentally relies on a verbalized tool-calling paradigm rather than true symbolic recursion.13

| Architecture Paradigm  | Operational Mechanism                                                 | Orchestration Interface                                 | Context Management Strategy                                               |
| :--------------------- | :-------------------------------------------------------------------- | :------------------------------------------------------ | :------------------------------------------------------------------------ |
| **Traditional Agent**  | Linear message history.                                               | Single threaded loop.                                   | Auto-compaction; token summarization.1                                    |
| **Claude Agent Teams** | Discrete sub-agent sessions spawned via explicit Task tools.21        | Verbalized tool invocation; parallel team structures.22 | Clean context per agent; manual task chunking.20                          |
| **True RLM**           | Symbolic manipulation of context via a REPL and llm_query function.23 | Programmatic for loops and map/reduce code blocks.13    | Zero native context footprint; data held entirely in environment memory.4 |

The Claude Code approach fails when confronting massive, systematic audits that require thousands of micro-analyses. As researchers have noted, a typical sub-agent strategy cannot express ![][image13] sub-calls as standard tool invocations because the model physically cannot verbalize that many explicit sub-prompts in its output stream.13 Thus, true recursion must be symbolic, operated through code, not tools.

### **2.2 Model Context Protocol (MCP) Wrappers**

Recognizing the limitations of native tool orchestration, developers have attempted to bridge the gap by implementing the RLM paradigm as an external Model Context Protocol (MCP) server. Implementations such as richardwhiteii/rlm and adrianwedd/rlm-mcp wrap a Python-based RLM logic layer within an MCP interface.6 In these systems, the Claude Code agent connects to the MCP server, passing queries which the server then executes against massive documents using intelligent chunking, SQLite persistence, and recursive sub-LLM orchestration.6

While this successfully processes documents exceeding 100x the standard context window 6, it suffers from profound architectural inefficiencies. The MCP framework necessitates heavy JSON-RPC serialization overhead over standard input/output streams.24 Furthermore, managing session persistence, file pointers, and artifact provenance across the stateless MCP boundary introduces immense latency and complexity.24 Most detrimentally, the execution occurs entirely out of view of the primary terminal interface, creating an opaque "black box" where the user is left waiting for prolonged periods without real-time observability.

### **2.3 The ypi Bash Implementation for Pi**

The most relevant prior art specifically targeting the pi-mono toolkit is ypi (Recursive Pi), an external wrapper developed by rawwerks.25 Unlike the Python-centric MIT baseline, ypi ingeniously utilizes standard bash as its primary REPL.25 The architecture introduces a single custom bash function, rlm_query, injected into the agent's knowledge base via the SYSTEM_PROMPT.md file.25

The operational mechanics of ypi rely heavily on heavy operating system primitives:

1. **Execution Model:** When the agent invokes rlm_query, ypi spawns an entirely new child instance of the pi binary operating strictly in non-interactive, headless print mode (-p).25
2. **State and Context Management:** The massive context is written to disk and referenced by a $CONTEXT bash variable, completely avoiding the LLM's token window.25
3. **File System Isolation:** To prevent concurrent child processes from destructively overwriting the working directory, ypi automatically generates an isolated jj (Jujutsu) version control workspace for every child execution. The parent agent then reviews the child's work utilizing jj diff and merges changes using jj squash.25
4. **Resource Guardrails:** To prevent runaway financial expenditures, the bash wrapper enforces strict environmental limits including RLM_BUDGET, RLM_MAX_DEPTH, and RLM_TIMEOUT.25

While conceptually brilliant, ypi operates as an external shell orchestrator rather than a deeply integrated native extension. The system forces all child processes into headless modes, fundamentally breaking the user's ability to utilize the sophisticated pi-tui interface to seamlessly monitor sub-agent thought processes in real-time.25 Additionally, relying on primitive bash tools (such as grep, sed, and awk) to execute highly complex abstract syntax tree parsing or data manipulation is profoundly brittle when compared to a fully-featured, statically typed programming language. Finally, the continuous orchestration of new jj version control workspaces introduces extreme disk I/O latency and computational overhead.

The mandate is clear: the RLM functionality must be implemented natively using the extension language—TypeScript—directly integrating into the terminal interface to solve both the performance and observability deficits of previous approaches.

## **3\. The pi Framework and TypeScript Extension Architecture**

To seamlessly integrate the true RLM pattern natively, it is critical to map the precise internal architecture of the pi-mono toolkit. The framework, created by Mario Zechner, is heavily modular, layered intentionally to allow extensive functional customization without requiring developers to fork and modify the deeply nested internal source code.27

### **3.1 Core Monorepo Layers**

The toolkit is divided into a succession of specialized Node.js packages 29:

1. **@mariozechner/pi-ai:** The foundational unified LLM API layer. It expertly handles multi-provider communication (supporting Anthropic, OpenAI, Google, Groq, and local models via Ollama or vLLM pods), manages real-time streaming, and executes token and cost tracking.28 Crucially, it natively supports TypeBox schemas for rigorous, statically validated tool definitions.28
2. **@mariozechner/pi-agent-core:** The underlying agent runtime that executes the fundamental while(tool_use) processing loop.21 It manages tool execution algorithms, validates parameter inputs, and streams detailed lifecycle events via an internal event bus.28
3. **@mariozechner/pi-coding-agent:** The primary CLI application that wires all core components together. It is responsible for orchestrating complex session management, multi-branch conversation paths, the native context compaction routines, and the dynamic loading of the extension ecosystem.28
4. **@mariozechner/pi-tui:** A sophisticated, highly optimized terminal UI framework utilizing differential rendering technology. Instead of relying on React-style asynchronous updates that cause visual artifacts, pi-tui wraps all rendering commands with synchronous output escape sequences, ensuring a perfectly flicker-free interface even during complex, high-frequency multi-component updates.28

### **3.2 The Extension API Surface**

Extensions in pi are TypeScript modules that are dynamically discovered and loaded at runtime (utilizing the jiti compilation engine to process TypeScript without pre-compilation) from defined global or project-local directories (\~/.pi/agent/extensions/ or .pi/extensions/).32 These extensions operate with full system permissions, possessing the capability to intercept deep lifecycle events, register novel tools, and inject advanced custom components into the user interface.30 An extension exports a default function receiving the powerful ExtensionAPI object.32

The ExtensionAPI provides a robust suite of critical integration hooks necessary for embedding the RLM architecture:

- **Tool Lifecycle Modification:** The pi.registerTool() function allows the extension to define new executable tools equipped with strict TypeBox parameter validations that the LLM can safely invoke.32 Furthermore, extensions have the privileged capability to override core built-in tools (such as read, bash, and grep) simply by registering a tool with identical nomenclature.32 If custom rendering logic is omitted, the override inherits the default syntax highlighting and diff rendering engines.32
- **Event Interception and Routing:** By subscribing to events via pi.on("tool_call",...) or pi.on("message_update",...), the extension can actively intercept, maliciously block, or transparently redirect tool parameters and stream tokens precisely before they hit the execution agent loop.32 The API also supports detailed lifecycle tracking via tool_execution_start, tool_execution_update, and tool_execution_end events.32
- **Persistent State Management:** The framework handles deep conversational branching via persistent JSONL session files. Extensions must align with this paradigm; they use the pi.appendEntry() function to write custom state data securely to the session track, ensuring that arbitrary state structures survive application restarts and accurately respect conversational branch navigation.32
- **TUI Widget Injection:** The ctx.ui.custom() method represents a monumental capability. It allows developers to inject complete, interactive TUI components (constructed from primitives such as Box, Text, Container, and Input) directly into the active session rendering stream. These injected widgets capture raw keyboard inputs asynchronously and trigger UI updates independent of the core message loop.32

## **4\. Architectural Blueprint: The Zero-Compaction TS-RLM**

The paramount, overarching goal of this architectural design is to ensure that the main pi agent _never_ triggers the system's native context compaction subroutines. In the standard pi configuration, compaction is forcibly triggered (or manually requested via /compact) when the linear sequence of conversational history and the raw text of loaded files begin to encroach upon the LLM's absolute token context threshold.20

By systematically routing all massive codebase reads, global directory searches, and complex log parsing operations away from the prompt and into an external, stateful TypeScript REPL environment, the main agent's working context remains strictly populated by high-level reasoning, dense summarizations, and virtual pointer variables.

The TS-RLM extension architecture is meticulously engineered across four deeply integrated software subsystems:

1. **The V8 Context Control Plane (The Native Environment ![][image7])**
2. **The Recursive Tool Suite (The API Interface)**
3. **The Sub-Agent Router (The Loop Controller)**
4. **The RLM Visualizer (The Interactive TUI Widget)**

### **4.1 The V8 Context Control Plane (![][image7])**

Instead of delegating execution to an isolated, external Python process via sluggish OS-level exec calls 10, or relying on fragile, stateless bash commands utilized by ypi 25, the environment ![][image7] is directly instantiated using the native Node.js vm module. This architecture provides an isolated, highly performant V8 JavaScript context where the main LLM can safely execute dynamically generated TypeScript code.

#### **Memory Management and Pointer Resolution**

Massive text artifacts, multi-gigabyte log files, or complete repository Abstract Syntax Trees (ASTs) are aggressively intercepted and loaded directly into the V8 context as native JavaScript objects, associative arrays, or raw memory strings. Because the V8 engine shares the underlying memory space of the host Node.js process running pi, passing a 50MB string into the sandbox is effectively instantaneous. This explicitly bypasses the standard Inter-Process Communication (IPC) serialization bottlenecks and JSON stringification overheads that critically paralyze MCP-based implementations.24

Within this control plane, data is assigned a specific variable identifier (a pointer). When the system responds to the main agent, it returns the pointer string (e.g., Context Variable: AST_Chunk_Gamma) rather than the raw data, preserving the strict zero-compaction paradigm.

#### **Injected Globals and VFS**

The V8 context is initially seeded with a suite of robust utility libraries essential for complex data manipulation, including Lodash for object iteration, advanced regex utilities, and a Virtual File System (VFS) adapter. The VFS overlay provides a critical safety mechanism; if the agent or a sub-agent executes logic that modifies a file, the changes are written strictly to the in-memory VFS. The root agent can subsequently query the VFS diffs and make an informed decision regarding whether to permanently flush the modified buffers to the physical disk.

Most importantly, the context is seeded with the asynchronous recursive execution functions: llm_query(prompt, data_pointer) and llm_batch(prompt, array_pointer).1

#### **The Zero-Compaction Intercept Layer**

To force the agent to utilize the V8 Control Plane, the extension radically overrides the default @mariozechner/pi-coding-agent toolset. Utilizing pi.registerTool(), the extension overrides the native read, grep, and find tools.32 When the agent inherently attempts to read a 200KB file, the overridden tool intercepts the execution call. It bypasses the standard file read, loads the massive file content directly into the V8 context, and returns a fabricated ReadToolDetails interface containing only a brief file summary and the newly established V8 pointer.32 The raw text is prevented entirely from entering the main LLM context window.

### **4.2 The Recursive Tool Suite**

To allow the main pi agent to interact intelligently with the V8 Context Control Plane, the extension registers a precise suite of custom tools.

#### **4.2.1 rlm_eval**

This tool provides the primary programmatic interface for the root agent. It allows the agent to execute arbitrary TypeScript code within the isolated V8 context to explore the pointer data.

- **Parameters:** Type.Object({ code: Type.String() }).
- **Execution Logic:** The extension securely evaluates the code block utilizing vm.runInContext. If the evaluated code returns a primitive value (a boolean, integer, or short string), it is returned directly to the agent. If the execution returns a massive data object, the tool automatically intercepts the return, truncates the output, generates a structural schema definition, and forces the LLM to write further iterative code to map, filter, or reduce the massive dataset.

#### **4.2.2 rlm_query (The Recursive Spawner)**

This tool acts as the primary symbolic recursive bridge. It allows the root agent to programmatically spawn a child sub-agent specifically tasked with deeply analyzing a designated chunk of data held in the V8 memory.

- **Parameters:** Type.Object({ instructions: Type.String(), variable_name: Type.String() }).
- **Execution Logic:** Upon invocation, the extension instantiates a completely isolated, ephemeral instance of the @mariozechner/pi-agent-core loop. It feeds the instructions as the primary system prompt and silently injects the specific data payload referenced by the variable_name pointer into the sub-agent's starting context.21
- **Return Mechanism:** The sub-agent's final textual output is returned to the root agent, or to rigorously preserve context limits, written directly back into a newly generated V8 variable pointer, allowing the root agent to continue its orchestration uninterrupted.

#### **4.2.3 rlm_batch**

Standard sub-agent strategies fail catastrophic performance checks when confronted with ![][image13] operations. The model physically cannot verbalize 5,000 sequential rlm_query calls.13 The rlm_batch tool facilitates massive parallel execution.

- **Parameters:** Type.Object({ instructions: Type.String(), array_variable_name: Type.String() }).
- **Execution Logic:** The extension maps over the designated array within the V8 context. It utilizes Promise.all to concurrently spawn multiple sub-agents via the pi-ai communication layer, passing chunks of the array to individual, isolated models simultaneously.1 It dynamically reduces the array of results and writes the synthesized data into a new variable in the V8 context, returning only a simple boolean success flag and the new data pointer to the root agent.

### **4.3 The Sub-Agent Router and Cost Economics**

Sub-agents fundamentally do not require the expensive, high-parameter-count intelligence of a root frontier model (such as Claude 3.5 Sonnet or OpenAI's GPT-4o).15 As observed in MIT's standard RLM implementations, routing deep, recursive sub-calls to significantly faster and vastly cheaper models drastically minimizes financial costs and execution latency while fully maintaining output accuracy, largely because the sub-task is narrowly bounded and highly specific.36

The TS-RLM extension utilizes the native ModelRegistry module exposed by the pi-coding-agent framework to seamlessly instantiate these sub-agents.30 Based on the complexity inferred from the instructions, the extension dynamically routes the sub-call to highly efficient models, such as Claude 3.5 Haiku or Google's Gemini Flash.36

This routing logic is fully customizable. The extension utilizes the pi.registerCommand() API to expose an interactive /rlm_config slash command. This allows the developer to seamlessly modify the designated sub-agent model mid-session. The configuration state is perpetually saved to the local session graph using the pi.appendEntry() state management pattern, ensuring the preference survives terminal restarts and seamlessly follows branched conversation trees.33

### **4.4 The RLM Visualizer (TUI Widget Integration)**

A fatal flaw in nearly all existing backend-oriented RLM architectures—including the MCP proxies and headless bash wrappers like ypi—is the complete eradication of system observability. The human user is relegated to staring blankly at a static terminal screen or a generic "Working..." spinner while dozens of autonomous sub-agents perform immensely complex, multi-minute tasks out of view.22 The @mariozechner/pi-tui framework was engineered precisely to solve this paradigm.31

Using the privileged ctx.ui.custom() API, the TS-RLM extension actively injects a highly responsive, interactive RecursiveTreeWidget directly into the terminal execution stream.32

#### **Widget Technical Specifications**

- **Interface Contract:** The widget directly implements the Component interface exported by @mariozechner/pi-tui, granting it full access to the differential rendering cycle.30
- **Structural Layout:** The UI utilizes a specialized Container module configured with a flex-row structural layout. The left horizontal pane renders a dynamically expanding tree structure representing the active RLM call hierarchy (e.g., Root \-\> rlm_batch \-\>). The right pane utilizes a high-performance Text component to cleanly stream the raw LLM thoughts and finalized token outputs of whatever specific sub-agent node the user currently has highlighted.34
- **Event Handling and Dispatch:** The extension actively listens to the tool_execution_start, tool_execution_update, and tool_execution_end lifecycle hooks specifically triggered by the rlm_query and rlm_batch tools.32 As the internal pi-agent-core streams partial results from the sub-agents, the event bus pushes these updates synchronously to the widget, updating the text stream in real-time.
- **Keyboard Navigation Matrix:** The custom widget natively implements the required handleInput(keyData: string) method.34 This implementation actively intercepts keyboard events. The user can press the Up/Down arrow keys (\`\\x1b
- **Synchronized Differential Rendering:** Because the pi-tui engine expertly manages the terminal bounding box layout and character padding constraints, the injected widget relies entirely on the framework's synchronous escape sequences. This ensures the UI renders the high-frequency chaos of multi-agent token streams cleanly and precisely, entirely avoiding the terminal flickering and textual tearing typically associated with raw asynchronous console output.31

## **5\. Execution Workflow and Application Lifecycle**

The seamless integration of these TypeScript components yields a fluid, fully observable, zero-compaction operational workflow. To demonstrate the technical efficacy, the lifecycle of a massive operation is detailed below.

### **Phase 1: Initialization and API Hooking**

Upon launching the pi executable in the terminal, the extension architecture is instantly auto-discovered via the local \~/.pi/agent/extensions/ directory.32 The default exported function receives the ExtensionAPI payload.

1. The module rigorously registers the RLM suite (rlm_eval, rlm_query, rlm_batch) incorporating strict TypeBox validations.28
2. It overrides the system read, grep, and find tools to intercept and trap massive context payloads.32
3. It initializes the isolated Node.js vm execution context, binding the VFS and llm_query globals into the memory space.

### **Phase 2: The "Seeking" Execution Protocol**

The human user enters an expansive prompt requiring comprehensive architectural context: _"Audit the entire 50,000-line src/ directory specifically for race conditions in the async handlers."_

1. The primary pi root agent assesses the sheer scope of the request and immediately generates a scoping program via the rlm_eval tool.
2. The generated script utilizes the overridden find tool to systematically locate all TypeScript files. Rather than dumping the text into the terminal, the extension secretly loads the file buffers into a V8 variable dynamically named codebase_ast.
3. The root agent, evaluating the size of the repository, invokes rlm_batch. It passes specific audit instructions and points the parameter array_variable_name directly to the codebase_ast memory pointer.

### **Phase 3: Recursive Orchestration and TUI Visualization**

1. The TS-RLM extension parses the rlm_batch request, mathematically splitting the codebase_ast into 50 logical chunks.
2. The controller dynamically spawns 50 independent pi-agent-core instances, explicitly routing the inference to the highly efficient Claude 3.5 Haiku model via pi-ai to minimize financial expenditure.36
3. Simultaneously, the ctx.ui.custom() widget instantly mounts within the terminal. The UI smoothly expands, revealing a 50-node interactive tree structure. As the remote sub-agents begin streaming their individual analyses, the TUI dynamically populates with data. The developer utilizes the arrow keys to visually monitor specific nodes checking specific files.
4. As the individual sub-agents reach completion, their final textual outputs are aggressively concatenated and persistently stored inside a new V8 pointer designated audit_results.

### **Phase 4: Synthesis and Zero-Compaction Finalization**

1. The rlm_batch background process completes, returning only a true success flag and the audit_results pointer to the waiting root agent.
2. The root agent, maintaining its flawless context memory, writes a final concise rlm_eval script to summarize the audit_results pointer, actively pulling only a highly refined, 500-token summary of the critical vulnerabilities into its actual working context window.
3. The main agent provides the final actionable report to the developer. The primary context window remains practically empty, effectively eradicating the necessity for context compaction, regardless of the millions of tokens physically processed inside the V8 REPL environment.

## **6\. Synthesis and Strategic Architecture Advantages**

The integration of the mathematical Recursive Language Model architecture natively into the highly extensible TypeScript pi ecosystem represents a profound paradigm shift in agent design. It orchestrates a critical transition away from the passive, linear consumption of immense context limits toward an active, programmatic "seeking" methodology.5

This specific zero-compaction architectural blueprint provides several distinct, insurmountable advantages over both traditional tool-use models and previous RLM scaffolding attempts:

1. **Total Eradication of Context Rot:** By relentlessly treating the codebase context as an external environment variable stored in native V8 memory, the root agent systematically maintains a pristine, highly focused token window. The agent mathematically never breaches the token threshold required to trigger Pi's native context compaction algorithms, completely preventing the systemic loss of nuance and reasoning fidelity.3
2. **Optimized Inference-Time Scaling Economics:** The architecture elegantly executes the economic trade-off of token context length for parallel compute time.14 Rather than processing 200,000 tokens through an extraordinarily expensive frontier model (costing significant capital per query), the system expertly routes localized, mathematically bounded chunks to cost-efficient sub-models in parallel. This mechanism achieves vastly superior analytical depth across sprawling codebases at a mere fraction of the cumulative inference cost.15
3. **Frictionless TS Developer Experience:** Unlike unstable, bash-based wrappers (ypi) or bloated, JSON-RPC heavy MCP proxy servers, the native TypeScript implementation directly embedded within the pi-coding-agent eradicates cross-language context switching and IPC serialization overhead.6 The data pointers seamlessly remain native JavaScript objects within the parent Node.js execution process.
4. **Unprecedented Sub-Agent Observability:** By expertly leveraging the advanced differential rendering engine of the @mariozechner/pi-tui framework via the ctx.ui.custom() widget API, the user gains a real-time, highly interactive viewport into the recursive multi-agent orchestration. This definitively solves the chronic "black box" observability problem that continues to plague nearly all background sub-agent systems currently in deployment.22

The proposed architecture effectively and masterfully bridges the theoretical and mathematical breakthroughs established by the MIT CSAIL Recursive Language Model paper with the pragmatic, deeply extensible reality of the modern pi-coding-agent framework. By aggressively offloading data structures to a native V8 environment, dynamically overriding core execution tools, and utilizing the advanced TUI widget API for unparalleled orchestration visualization, the software system transforms the coding agent from a linear, context-bound text processor into a highly scalable, hierarchical reasoning engine capable of operating autonomously on software codebases of theoretically unbounded size.

#### **Works cited**

1. Recursive Language Models: the paradigm of 2026 \- Prime Intellect, accessed February 22, 2026, [https://www.primeintellect.ai/blog/rlm](https://www.primeintellect.ai/blog/rlm)
2. Going Beyond the Context Window: Recursive Language Models in Action, accessed February 22, 2026, [https://towardsdatascience.com/going-beyond-the-context-window-recursive-language-models-in-action/](https://towardsdatascience.com/going-beyond-the-context-window-recursive-language-models-in-action/)
3. RAG Might be dead? How Recursive Language Models Scale LLMs Beyond Context Limits, accessed February 22, 2026, [https://medium.com/@ajjaiswal5.imp/rag-might-be-dead-how-recursive-language-models-scale-llms-beyond-context-limits-0b7eedbead81](https://medium.com/@ajjaiswal5.imp/rag-might-be-dead-how-recursive-language-models-scale-llms-beyond-context-limits-0b7eedbead81)
4. Recursive Language Models | Alex L. Zhang, accessed February 22, 2026, [https://alexzhang13.github.io/blog/2025/rlm/](https://alexzhang13.github.io/blog/2025/rlm/)
5. Recursive Language Models: Scaling Reasoning Beyond Context Windows | by Dr. Harsuminder Kaur Gill | Jan, 2026, accessed February 22, 2026, [https://medium.com/@harsuminder/recursive-language-models-scaling-reasoning-beyond-context-windows-d923b1d6d691](https://medium.com/@harsuminder/recursive-language-models-scaling-reasoning-beyond-context-windows-d923b1d6d691)
6. recursive-language-model · GitHub Topics, accessed February 22, 2026, [https://github.com/topics/recursive-language-model](https://github.com/topics/recursive-language-model)
7. Recursive Language Models \- arXiv, accessed February 22, 2026, [https://arxiv.org/html/2512.24601v1](https://arxiv.org/html/2512.24601v1)
8. What are Recursive Language Models (RLMs)? \- Rise Data Labs, accessed February 22, 2026, [https://risedatalabs.com/blog/recursive-language-models](https://risedatalabs.com/blog/recursive-language-models)
9. RLM: The Ultimate Evolution of AI? Recursive Language Models, accessed February 22, 2026, [https://www.youtube.com/watch?v=JF13pSE0KLA](https://www.youtube.com/watch?v=JF13pSE0KLA)
10. alexzhang13/rlm: General plug-and-play inference library for Recursive Language Models (RLMs), supporting various sandboxes. \- GitHub, accessed February 22, 2026, [https://github.com/alexzhang13/rlm](https://github.com/alexzhang13/rlm)
11. A Theoretical Framework for OOD Robustness in Transformers using Gevrey Classes \- arXiv.org, accessed February 22, 2026, [https://arxiv.org/pdf/2504.12991](https://arxiv.org/pdf/2504.12991)
12. Recursive Language Models: The End of Context Rot | Medium, accessed February 22, 2026, [https://medium.com/@cristianleo120/recursive-language-models-the-end-of-context-rot-649fc51885ea](https://medium.com/@cristianleo120/recursive-language-models-the-end-of-context-rot-649fc51885ea)
13. feat: Enable programmatic sub-LLM calls for RLM (Recursive Language Model) pattern · Issue \#8554 · anomalyco/opencode \- GitHub, accessed February 22, 2026, [https://github.com/anomalyco/opencode/issues/8554](https://github.com/anomalyco/opencode/issues/8554)
14. Recursive Language Models (RLMs): A Technical Deep Dive into the Infinite Context Paradigm \- Medium, accessed February 22, 2026, [https://medium.com/@comeback01/recursive-language-models-rlms-a-technical-deep-dive-into-the-infinite-context-paradigm-85b5b43373fc](https://medium.com/@comeback01/recursive-language-models-rlms-a-technical-deep-dive-into-the-infinite-context-paradigm-85b5b43373fc)
15. Claude code setup as an RLM scaffhold Implemented by Brainqub3 \- GitHub, accessed February 22, 2026, [https://github.com/brainqub3/claude_code_RLM](https://github.com/brainqub3/claude_code_RLM)
16. Claude Code overview \- Claude Code Docs, accessed February 22, 2026, [https://code.claude.com/docs/en/overview](https://code.claude.com/docs/en/overview)
17. Agent Teams with Claude Code and Claude Agent SDK, accessed February 22, 2026, [https://kargarisaac.medium.com/agent-teams-with-claude-code-and-claude-agent-sdk-e7de4e0cb03e](https://kargarisaac.medium.com/agent-teams-with-claude-code-and-claude-agent-sdk-e7de4e0cb03e)
18. Best practices for Claude Code subagents \- PubNub, accessed February 22, 2026, [https://www.pubnub.com/blog/best-practices-for-claude-code-sub-agents/](https://www.pubnub.com/blog/best-practices-for-claude-code-sub-agents/)
19. 7 Agentic Patterns for Claude Code \- visual reference with diagrams : r/ClaudeCode \- Reddit, accessed February 22, 2026, [https://www.reddit.com/r/ClaudeCode/comments/1p6bwyu/7_agentic_patterns_for_claude_code_visual/](https://www.reddit.com/r/ClaudeCode/comments/1p6bwyu/7_agentic_patterns_for_claude_code_visual/)
20. Best Practices for Claude Code, accessed February 22, 2026, [https://code.claude.com/docs/en/best-practices](https://code.claude.com/docs/en/best-practices)
21. Agent design lessons from Claude Code | Jannes' Blog, accessed February 22, 2026, [https://jannesklaas.github.io/ai/2025/07/20/claude-code-agent-design.html](https://jannesklaas.github.io/ai/2025/07/20/claude-code-agent-design.html)
22. Claude Code Multi-Agent Orchestration with Opus 4.6, Tmux and Agent Sandboxes, accessed February 22, 2026, [https://www.youtube.com/watch?v=RpUTF_U4kiw](https://www.youtube.com/watch?v=RpUTF_U4kiw)
23. Recursive Language Models in ADK \- Community Articles \- Google Developer forums, accessed February 22, 2026, [https://discuss.google.dev/t/recursive-language-models-in-adk/323523](https://discuss.google.dev/t/recursive-language-models-in-adk/323523)
24. I'm convinced Code Execution and RLM are the exact same thing : r/mcp \- Reddit, accessed February 22, 2026, [https://www.reddit.com/r/mcp/comments/1qo04oa/im_convinced_code_execution_and_rlm_are_the_exact/](https://www.reddit.com/r/mcp/comments/1qo04oa/im_convinced_code_execution_and_rlm_are_the_exact/)
25. rawwerks/ypi: A recursive coding agent inpired by RLMs \- GitHub, accessed February 22, 2026, [https://github.com/rawwerks/ypi](https://github.com/rawwerks/ypi)
26. RAW.works, accessed February 22, 2026, [https://raw.works/](https://raw.works/)
27. mariozechner/pi-coding-agent \- NPM, accessed February 22, 2026, [https://www.npmjs.com/package/@mariozechner/pi-coding-agent](https://www.npmjs.com/package/@mariozechner/pi-coding-agent)
28. What I learned building an opinionated and minimal coding agent \- { Mario Zechner }, accessed February 22, 2026, [https://mariozechner.at/posts/2025-11-30-pi-coding-agent/](https://mariozechner.at/posts/2025-11-30-pi-coding-agent/)
29. Nader's Thoughts | Nader Dabit | Substack, accessed February 22, 2026, [https://nader.substack.com/](https://nader.substack.com/)
30. pi-mono/packages/coding-agent/src/core/extensions/types.ts at main \- GitHub, accessed February 22, 2026, [https://github.com/badlogic/pi-mono/blob/main/packages/coding-agent/src/core/extensions/types.ts](https://github.com/badlogic/pi-mono/blob/main/packages/coding-agent/src/core/extensions/types.ts)
31. Pi-mono: The Minimalist AI Coding Assistant Behind OpenClaw \- Medium, accessed February 22, 2026, [https://medium.com/@ai-engineering-trend/pi-mono-the-minimalist-ai-coding-assistant-behind-openclaw-bd3ccc0a1b04](https://medium.com/@ai-engineering-trend/pi-mono-the-minimalist-ai-coding-assistant-behind-openclaw-bd3ccc0a1b04)
32. pi-mono/packages/coding-agent/docs/extensions.md at main \- GitHub, accessed February 22, 2026, [https://github.com/badlogic/pi-mono/blob/main/packages/coding-agent/docs/extensions.md](https://github.com/badlogic/pi-mono/blob/main/packages/coding-agent/docs/extensions.md)
33. pi-mono/packages/coding-agent/examples/extensions/tools.ts at main \- GitHub, accessed February 22, 2026, [https://github.com/badlogic/pi-mono/blob/main/packages/coding-agent/examples/extensions/tools.ts](https://github.com/badlogic/pi-mono/blob/main/packages/coding-agent/examples/extensions/tools.ts)
34. Session Export \- 2025-11-20T23-32-57-223Z_d703a1a9-1b7b-4fb1-b512-c9738b1fe617.jsonl \- { Mario Zechner }, accessed February 22, 2026, [https://mariozechner.at/posts/2025-11-30-pi-coding-agent/media/long-session.html](https://mariozechner.at/posts/2025-11-30-pi-coding-agent/media/long-session.html)
35. @mariozechner/pi-tui \- npm, accessed February 22, 2026, [https://www.npmjs.com/package/@mariozechner/pi-tui](https://www.npmjs.com/package/@mariozechner/pi-tui)
36. egoughnour/massive-context-mcp: Handle massive contexts (10M+ tokens) with chunking, sub-queries, and free local inference via Ollama \- GitHub, accessed February 22, 2026, [https://github.com/egoughnour/massive-context-mcp](https://github.com/egoughnour/massive-context-mcp)
37. Create custom subagents \- Claude Code Docs, accessed February 22, 2026, [https://code.claude.com/docs/en/sub-agents](https://code.claude.com/docs/en/sub-agents)
38. Why Your LLM Keeps Forgetting Things (And What MIT Just Did About It) \- AI Advances, accessed February 22, 2026, [https://ai.gopubby.com/why-your-long-context-llm-keeps-forgetting-things-and-what-mit-just-did-about-it-b2c8e2a696a4](https://ai.gopubby.com/why-your-long-context-llm-keeps-forgetting-things-and-what-mit-just-did-about-it-b2c8e2a696a4)

[image1]: data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABUAAAAYCAYAAAAVibZIAAABsElEQVR4AeyTOy9EQRTHx6MQQaLwCCIeHRVCovCqNKIREh9ApdMqJFq+gehUfAAqQYQIakI8Eo9oJBoR8cj+/ntndu+dvdnb7Ha7+f/mnDlz9ty5586UmyL8SkUjTR1gdgdvlhNsPk2x6HKf8bfB+D29JNgNe/ANg1AJcaohuAhNcAbtMAc5RRUTfQx6qgoqmWmOlokcg7TL8A9p+TtVsJHhA65A6tTg0c/8E5pB2tfgiCs6yuIhPILkF60guARrMA6vcAMZxRUdY/UAXNEu/LDUxw0CddALysVkFVd0mOVTeII/6ACnNhx9SL2JHl7GPLFoA0nq1Rf2B3RMwjtdIbYKkl5dNrGonn6kTIta4Ho6Q0w7fMdKKvqCcwsR+a8/war+iEnrgVGnoQU7DVsgKdaDE85lGsgvOkI4fItUlJDZZHCvjWu0S/UzsaiuXL0x5hec7q2jluj62qmZtI7i1s0a7VT3Xb3RDdKHUs9mbco19hzWQdJRUp/nmega6yPt4EekorrvrUSroQpqwSVe4A+BTgLGLDDoiLlc3Si3AZYCqWjgFXAsFS1gM22povQ0BQAA//8mK6UsAAAABklEQVQDAOuLSTEqGa1AAAAAAElFTkSuQmCC
[image2]: data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAkAAAAZCAYAAADjRwSLAAABBElEQVR4AezROy8EURQA4PWIQqGhFQokGgWRiJDQKPwGjYSoVKLRaDUSiVeh9BsUSo9VCC2iUIjEI1FLCPGdycxkJ6aQrXdzvnvPuXtm5s6d5so/fo2mSl1HsORsr7lkjUU2au90YGGZGSaYYp+HrGlescAK73xyRAvVaGqVrPPEMVmMSOKC22gaVvRwSm1MKqr8RFO/JOI8hlS3uZczkiN4jYQ7sphNk7zpxMIb47QRb7dr/uCK5E5fklHiDZ/NY9xzwTdJU8yPhj462WOQfI+xcXUhptOqtKndn3OsEjFkGCB/XOQdhi4Oic/TZI6DLjS9WNximx02uaHQFHWpso3/afwFAAD//z7ispEAAAAGSURBVAMA8C8rM8q14GsAAAAASUVORK5CYII=
[image3]: data:image/png;base64,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
[image4]: data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAEUAAAAYCAYAAACsnTAAAAAFKElEQVR4AeyYZ6hcRRTHr703sPcGNuwdsWP5IFaiKKIi2FBsiCBW1A82RARFrB8UO4KCDew1IZWQHlJIJz0hIb38frs7+2bnzt48yNtNPuTx/99z5szcO/eeOXPm7Nu62PJX8sAWp5RcUhTddMpBmfk3lWk/Jt4OZtEtpzzF7NfCbmArJjkUXgbPh/HHH0NbKL9A2QaWkDrldEZMgLMa/A9ZhavoDGOnoX8JU1yH4WT4FuwkduXhz0Df41tkP3gPHADPgI/Dh6H4i8tA+CYsIXXKYEYcBX+CK+CZcFuYgy9xPx2GohO7OjfSjnEAjfdheBnUjuAUnjoc3gJvhqfCu+Gt8Hr4NXwZ6gxEDa9wvQIaUYgepE4JPT7UVdchfmywx/JJGv9A8SOXtTDFvRh08HRkp3A0D3aOqUidE384pmJyURR+C6L4o+j5W4NqpLyAbEHOKfsyYiEcDcURXhKeRnsp3B+K37wk3IG24fspslMwSn/m4S7ITcjlMIeRGMfCmTDG5zTOhgYBoo6cUy6g60+ohxFF6hST06N0vAovgjPgOJjCJKeD/087oraR6EtdiM2EuBPyWNhbPMvAI6GrbW5DzWIl1t9hitkYxsNLYRM5p/iChllwipM2b0Axj7yH3B2eAB2LKMHkZtKeX+qpG4w2k52n0uWYTI7mpovRewOP+DsZuAh+DKtgNHkC5sYMwugCIurIOeVculxd96j77nDaAQejmIiNJJ3n8dfOKe51V4JbSjgOiyvnXn8C3fzklj0R/W/YG3jyuUWdf8kGbnBh5rUZ4zseGPelTtmHTnPFMuQq6PEWR4rh+jx24dZR+lLKlHtiWABz+ACj87yODHABfHH3f7BVSR1r/xAvG0Hfce/4/tQprn6cvd1CIafcwI1GiC+OWugUTxX3ZJH5cxU91tOuszAYjb8i4/4w9zrsvUH4EBNo1fiT6HwItoPvsHPcmTrF/eyHhzGTUEyWhtfV6J9Aoe14lHgszRYYsnu0WOqN8+qiiO81Gg/BHttoVsJ85QCjW9mOD9LhAiCy8B0Xxz2pU0w4cRWrUxxvuIdtY9soMZ9UfcRcBu4FU3iftn+9NOjzVNPneVTuYkeGnzVsRlhDjURdNYEbCSPqzezVbT4n7omdYuLyI1ZHAyY2dLdUWBlNVoJK7cocrXOMgHgOxwWnh/A3cVueG1lWpY6Rd3AxX7yDzGEMxjeg29rKFbUF/WjdBR+BVfBAGBoP8IX9vWNu8CQwFM0ZPtBxTuyx+ZoN6FFsnrGUdgVMsl9hz8GQdRVCQgxj+qO8CE2ybyOfg25HTx2LMJo1WPtYe5xTa+Uvj2H2We8ira7Vfa4Vts62oLNGoTsLay7rJO9tDtAp/t7xzDfZ7EjPbjB8qGe4idGTCHOh5z2iw1gr2uBA+2O69TxJLomNDf1ppAWTztYpFm3p1jGi/O00hbHt4IlllB3GAAs4o/kH9GvgSzB2Ms0SDAir4G/iHp0St/tad+Xu46Ehj6A2YXJze+ocjalTtLlY1kvqVTQnWKB9xKBfYFV00N2EhahRZtQ3jZ12ilvSwsqVa07aUEygHs3mA/OY0eJ2a3TXhDlD1hp9fLHUcP7S8zvtFMPXMt4S260Qf5cniyvlNvMkeYBOK2ZEDR6Vlv7Daq2+vWzP4z6E5hxPSdQedNopzuQPxttQ/GcTogmTm1FyOxb7Td7x0elvmlAXMaRPcSVPM5+1nDrYauiGU5xoFBdPGsRmge94i+9hFt1ySnbyzdW4HgAA//9kpuvuAAAABklEQVQDABRP7TGo0ezAAAAAAElFTkSuQmCC
[image5]: data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAwAAAAXCAYAAAA/ZK6/AAABKElEQVR4AeyRvUpDQRBGryKolVqqYKedggqCgo1iZWNhYyModj6KhQ9gLYIP4E+hNoKFYKOFCELKFAkhJIFAyM85C7ukS9KlSPjOndm5+2V35o5nA/5Ghn4GFqc0weZ5mAY1xmMWFmASkjT48oDKIZyDm/aIu3AEZzADQRqWyI6hCVdwCavQgCpcww4Eadgie4J/8EpzxBuw9kycghUI0uDGV1ZeoU70lBpRrfuAXwjS8EVWhH3wRY4YZQ95Fp8QpMHEiWyQvEEblI06iEcWZTiFLBo2WbjhhRi1RrII97ANy5AMNl6g8AFR/muFhX140h15MnjsBYUSRP2RnID3vyXaXzJ8U3iAeH/SzIk51ncWP9CCZPCj+aGsdWNdY6rFplOhVzKEhg4AAAD//2V9P8AAAAAGSURBVAMA8zUzL1+rrTUAAAAASUVORK5CYII=
[image6]: data:image/png;base64,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
[image7]: data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAwAAAAXCAYAAAA/ZK6/AAABNklEQVR4AczSsStFYRQA8EuIgSil/AEWNmQ0YDGgkBLFJKt/wKIIZWE3ICkDsRpMBiYDFiFZRCkbye/cenr3vbeYuJ3fPd89p9P9+u4tT355/e1Apd22M0AbaZTaUrXOPE+cc8AlcySFA42KZzTTRS0LRDTErXBgVzG2Mivf8s4VW6ySeUOrQg93fJGLHYtJ3sgMfEaBXkYpoyjyt3Sju0EVezyyRCbyB6Jx6vbKIeuskIn8gWWdOLpOeZBFXshEbmBKNU5mTI7TkUpHDNRprRH7vpcLI/rpN4hGDHRb1BN7lzLR5+maJtKIgYp0lSQzcj9xnDVynP22PEEMSUn6HY6s9onf4FiOr/ssD9HBCT8Rb/jwNEI0p+VxWhjmgUzEQK5wYbFJ/J3xp1oWR/5AcbdE5R8OfAMAAP//R6mQWwAAAAZJREFUAwDvrjEvhhRwgQAAAABJRU5ErkJggg==
[image8]: data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAA8AAAAYCAYAAAAlBadpAAABU0lEQVR4AeTTzSvDcQDH8XFACRclF5KUC3cpRxQ35eYPIOXCTSkXhcRB3DyUpzg6uJFwkFaKgzxvt7XddthTa+/P1vfXb9vv99tql9XW57Xv7/vw2fPqfRXcqrfcyLsaxCT6YNLGRafby1bhnAMhrGEcu7hEB27Q5VReZOMFSfRiAvMYxQM+MQB/YVmHNthYwAzCsEfPHmXhEUl7eZqFHRxgD05JsfiBO/hMuZmJHjXOuASvJNi8hVWeZdKOU0TglTk2n2GVpzTBNUrlmwNpWOV+TeBH2dF7buC0vvQYYwBeWWGzG9morA8gyKwJLXBLKxvD+Ec2KutCH5TGEd05qGNtG6uwYsqbrHxhCz2wR69mn4V76BfGkIsp6+sZY+kPr7jAMo5xhiMcIi+mrMUf7vQAQ4xX+MU69I96YiyKvWw237nQv+eE8Q2ucSq7Hi7cqMVyBgAA//+wBijcAAAABklEQVQDAJlfOzF8pkPPAAAAAElFTkSuQmCC
[image9]: data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAwAAAAXCAYAAAA/ZK6/AAABEklEQVR4AezQLWhCURjG8buFwT7C2Mq2sLSFla2OjbWF5cFY32BMMWky2sUsFsEqFqvFbFfBoAZBMIviJ/6fq6/cCyLaDMr7O+85R5577zmHzpa/fWCTC/Pe0gGBE5zibIUj9hwLXLD4xD+CCEDdaP3BnhvQ035ZfOMNUbzgHSE8L9zR3cADk0vEEUMTEaRRhOaSZO4GukwSKOEWZdRxjRoa0EN6dDdQZdKBDv1DL0D1xDCGr+zQ2nxk0LfrM5g6NwxX8JU38Mc/enWLrpowvEJvps3LAucsv5DDFKoKwz2OsSwL9NkJIwOrFBNd95C+LAsM2MmiDSvdUJ6F7+AWYN9RSN3o00a2sO4N2N7avoOBGQAAAP//r2TjwgAAAAZJREFUAwDRIywvgGEb3AAAAABJRU5ErkJggg==
[image10]: data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAkAAAAYCAYAAAAoG9cuAAABRUlEQVR4AbySvUtCURjGb1JLVEMRLUFTUUMNBQVB0RJEtLQ19S80VxB9UGv/QF80uQgi7g5+IA6CiDg4KDjpoIi4+fV7Lh7xXq7g5OX9nfc573nuOeeec33WBM90TDNmJ+OWO8XwC7NgeZmOGQjDGvyBz23ap+iHDbiEMny5TRmKO1CHHjzAndvUodgCEzI23aYrRpMQh0+4gJ9R0z2Fb7gBfd0yOQQVYzqj8wbPUAItGyDPQdyYXulo4J9s4gDRhYRMq4gTSMPoplXLUavJtIVQRNUM0DJHaLsmU5WOIq9mgGaZR8fAvpYCIguHoLvaJgdBMTSpozfPEU3QKeucimhdiz0T2mrQbMIi3IIu2Z4FPTRJ6wraCH36EtnTpJ/smsEnUKzT7IFjpgUKu5CCR9DhrpAdJm1aJ//OwAe8QAQcJvU96QMAAP//lnx+ewAAAAZJREFUAwArjToxXMsVwQAAAABJRU5ErkJggg==
[image11]: data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAA8AAAAYCAYAAAAlBadpAAABiElEQVR4AdSUPShGYRTHL4kysygDEykyGJSSTBiICavNYKZEMRuIxCAGYvGR0VcZFFIUoQxSFkaDj8Tv/7rnca/ufVi9/X/3PM8553973+ee+2YH/k8W5TFYgzyIyWeWcZzuIjiFdcgHJ5+5ga4n6IQhWIIecPKZd+jqhw+Q5rnomxC+5DOrI4dLGTRDJZhyWZSmmUsozsAjTEILjMAW6AxWiDVJ5i4KZ1AMVdAIvdAKE3AJWu//NLdRWIApaIJbiEonLvM1yfuouY6ETnSXOABpOqegnsDMitMkNQh9xHdI0yuFPXDmdjYVoMdzQfRplOIqOHOHNrAJv+mOhhdw5nJt4AT+LP1WNRfoAlfgk8az1hrMfBMmCsOYFDRt3RSOISMzL2Z2QVAfxqQwTFJP5I2YkZk19Adk9PZUE6PSHOuEn0kug5OZVdDIbVPRTTRJg6xnYQMOQTcgfMvMyjxw0bur5z3HWqOpl0NjqhuQiitqtooOT387mvEjkvY+s4wryRzv8Oz+qfkTAAD//+aKjAUAAAAGSURBVAMABL5DMXxKM98AAAAASUVORK5CYII=
[image12]: data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAwAAAAYCAYAAADOMhxqAAABdUlEQVR4AcyTvStGYRiHj6/CoCz+AZOPFLIZ5KN8FAtZJCYLBoMsJLsSWSySksEig5hYUQaDUJTyUQalRBm4rlPndY73vLu33/Xcz30/z+8959znOflB7l8zSzdQBxnlMrSwYw0WYBuaIFSaoYiVcWiHTeiBaciDIM3wxUI/PIK6YxiAb0g1WJcChlrohUYI/zwcSOKyNknhFi5gF85gCbKuUELxENqgE0phDFS5g/9mjFhh0gCDcAkfcAV7MAOJK5RRGIUn+IRIx0x8jgdiwmBuh6qYTIAPTUgqfktvLM2B7VsmPsMGJBQ3uHDK8AIHsAqzkFDcYCvd1MUOOzRPvIeEIkMr1UUYgXPIKQ2eEQ+a3ThJ2VlMrQJCafD1V5K9wl/VU7iGagiloTCcBUEf0VuynZ7YbvJ98IUdEUNp8J59wxrXqb6DV5sidsAWZKTBxA7VMBmGIfAr83vw8JH+KjJY8ez4weyQeFIJ2YobsldTKv/Q8AMAAP//mL+QBQAAAAZJREFUAwBcOzsxcus+BwAAAABJRU5ErkJggg==
[image13]: data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAYCAYAAAC8/X7cAAAD70lEQVR4AeyWR6gVSRSG+03OOTGLCczA7IaJi8ljQDEi6EZxIWZFRDAjmFBQFyKKYNoJooJZQcGcMaEIBhTMmDNiDt93vdX27dt9Hz7fWwhe/r/OqVNV3fdUnTqnX4qe898LBzIO8CNsb8Hawjs86AOYiepO4A1W/Qebwy9hdXDOPCa9CmsLbsZiHvYFLEOeAz8w00VLkTrwO3I1XAE/h1l4HeNs2B9ehWnsxHAKnoHHoM4iYgxAOw4PF7kP+S48B4fABVBnEE+QdqCKocFwHZwOG8KRcBj8Gb4Jt0KPFVGC0fT2wh0wC79i7Aw91a+Q3WAS4+g0gO50d+SP8DoUG2gOQB1BPEHagfEMDYUt4RKYxE06OvcNsi9M4n06XeBEWAmeZj8m3IE68BoyiZfpuHkrkfdgEhPo9IafwhhJB1ph7QNHwW0wC+7uQwZawCQ60DlYJCIXfzMyF3pPDMU26En8Q2ctzMIejIZee2SM4IC3fDLWK1BPEZm4j1UHvkcm4R/ZnDRk6IadIXqNsUlQ9LJJ8F/0PAcYinyHYaZeYHCgIz0v1SzkDZgHY9c1OhLmeOzeD08n2LLkXxg3QrGFxkv9B/IXGOBzdoVOhvQdnlI85J+x09QGLoeV4J9wfL9NkTr+NrrZApGL/xkxvhEFpE/BUz3CSHJz6JbAd7yHxdNERJEOeJGCV9sL1vwmxH7yght+rrhsU4G+I5yA0+bQnIdt4cdQByuFD1MiQzzi53zEYweq0F6BD6APRGTiM6xmJ3P8DPQAN0D9lk0OzedulvEfpjjfVG1aNb3WY2ANrATXOB7XAx96G8tRqF6pgloLLFYWqovMD7hUVEylRbVMmH28gOmBKRhMlz2QP8HdsBLCO+KN8E+7IBytxcZ+ms0wWFx8obtGN8aFovZhUWaJ+hjXwzROYFgEv4aHoFGAyIXvcE7YtMIdcPYgGmPYXTar0I3RGs1PBCtlOu0xFFktT6J4CRFlMHOZZv2EKBvEEC5zdfHP1Og7GiuyRRU1ih04Ta8xtIx71FY8q+0qbFbMJkircF6GcN6fzEmiio4hYYGzaPkHp2FLw8xkSvUZ6bF037TrZ0VsDyGkwQxkCPWk42noaSf0RjCEGGomlmE1y5gMUAuw4BnXfj956WTXwkh58xsmv6MQufD+uUklm5B0wJV3adyNmUj/lJcbtVrMZ4YX23BDrRO046l+oZYUurQDzKkRDC3vkRkqfYdq9MDUIlO1YT0wZY/vQNpek77FbSELh8PaxlgeaAbchCzBU5xAybq8jl+y5vVP8ibUwP4ta87CqbAMte2ALxhBE2oD6jPD76MxeU+pCwfy3lUn9kcAAAD//5MjeLEAAAAGSURBVAMAfXKqMX10+0gAAAAASUVORK5CYII=
